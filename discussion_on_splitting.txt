Joshua L. Phillips
Binning is different from what we are doing in the lab, and is an entire lecture in non-parametric approximation in itself. The method we are using for the lab is virtually model-free in that all possible binary divisions are considered. For binning, you will have pros and cons for any particular approach that you take.
7 replies

Jared Frazier  14 minutes ago
I see. For the binary split, I assumed we would have to bin the data according to some information gain. I was basing mine off the reading in "3.3." (pg. 9) of An Implementation of ID3 -- Decision Tree Learning Algorithm by Peng et. al.
Is my strategy still an appropriate one for this lab? (edited) 

Joshua L. Phillips  7 minutes ago
No, you have to follow the lab specification: sort each attribute and find all potential binary split points. This approach eliminates the need to be concerned with binning, and adapts to any data in a straight-forward manner for a little extra computational cost. This isn't really binning because we don't break the attribute into discrete buckets. Instead, you just create two "buckets" for each split point (when an attribute changes value in the sorted set of examples): one for less than v and one for greater than v, where v is the average of the two values (which are different). However, each pair of buckets is unique to the particular change in value observed along a sorted attribute. (edited) 

Joshua L. Phillips  4 minutes ago
Each potential binary split point has an info gain value that can be computed for it: we just choose the one out of the entire data set which provides the largest gain. This creates then, two data sets: one for the less-than branch and one for the greater-than branch. The entire approach is recomputed again on each of these subsets independently.
white_check_mark
eyes
raised_hands




Jared Frazier  3 minutes ago
Oh.... this seems way simpler than what I was trying to do. By "attribute changes" do you mean the "value" of an attribute changing? e.g., sorting an attribute (feature) vector like temperature -> {31, 70, 85}, then since there is change in value from the first to second element so one binary split point would be (31 + 70)/2? Would the other be (70 + 85)/2? Then you determine which binary split for the attribute has the highest information gain and go from there?
Then you do this process for each attribute in the learning subset to determine "buckets"? (edited) 

Joshua L. Phillips  2 minutes ago
Yes, there are two potential split points there. But you need to search across all other attributes as well. Once you have: only one binary split point will have the highest gain (or ties are broken by preferring splits using left-most attributes and/or smaller values).

Jared Frazier  1 minute ago
Oh dear. Yea, I way overthought the splitting strategy here. Thanks for clearing it up.

Joshua L. Phillips  1 minute ago
This is chosen, and a split node is created. Then the left and right children can be passed the newly split data sets (one set to the less/left, and one set to the greater/right).